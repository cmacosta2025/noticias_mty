{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a0b79b-3621-4313-b26c-0c552c64c201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.30.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in ./venv311/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Using cached trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in ./venv311/lib/python3.11/site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in ./venv311/lib/python3.11/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in ./venv311/lib/python3.11/site-packages (from selenium) (1.8.0)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.2.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (116 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv311/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: requests in ./venv311/lib/python3.11/site-packages (from webdriver-manager) (2.32.3)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: packaging in ./venv311/lib/python3.11/site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in ./venv311/lib/python3.11/site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in ./venv311/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in ./venv311/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv311/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in ./venv311/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.30.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading numpy-2.2.4-cp311-cp311-macosx_11_0_arm64.whl (14.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, pytz, wsproto, tzdata, python-dotenv, pysocks, outcome, numpy, webdriver-manager, trio, pandas, trio-websocket, selenium\n",
      "Successfully installed numpy-2.2.4 outcome-1.3.0.post0 pandas-2.2.3 pysocks-1.7.1 python-dotenv-1.0.1 pytz-2025.1 selenium-4.30.0 sortedcontainers-2.4.0 trio-0.29.0 trio-websocket-0.12.2 tzdata-2025.2 webdriver-manager-4.0.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium pandas webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43a61ad-35ce-409c-b66b-217464367bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in ./venv311/lib/python3.11/site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in ./venv311/lib/python3.11/site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in ./venv311/lib/python3.11/site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in ./venv311/lib/python3.11/site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv311/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv311/lib/python3.11/site-packages (from requests->webdriver-manager) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv311/lib/python3.11/site-packages (from requests->webdriver-manager) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv311/lib/python3.11/site-packages (from requests->webdriver-manager) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad4462a-00ef-4165-98d3-ba11054e67f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Procesando noticias de: Monterrey\n",
      "ğŸ“… Ãšltima fecha registrada: 19/03/2025\n",
      "âœ… DÃ­a 20/03/2025 procesado (47 noticias).\n",
      "âœ… DÃ­a 21/03/2025 procesado (34 noticias).\n",
      "âœ… DÃ­a 22/03/2025 procesado (20 noticias).\n",
      "âœ… DÃ­a 23/03/2025 procesado (29 noticias).\n",
      "âœ… DÃ­a 24/03/2025 procesado (9 noticias).\n",
      "ğŸ“¥ Se agregaron 139 noticias nuevas a Monterrey.\n",
      "\n",
      "ğŸ” Procesando noticias de: Adrian_de_la_Garza\n",
      "ğŸ“… Ãšltima fecha registrada: 19/03/2025\n",
      "âœ… DÃ­a 20/03/2025 procesado (8 noticias).\n",
      "âœ… DÃ­a 21/03/2025 procesado (5 noticias).\n",
      "âœ… DÃ­a 22/03/2025 procesado (1 noticias).\n",
      "âœ… DÃ­a 23/03/2025 procesado (5 noticias).\n",
      "âš  Error extrayendo https://www.elhorizonte.mx/nuevoleon/impulsara-mauricio-fernandez-reformas-contra-los-chocones/4759802649: Invalid \\escape: line 35 column 5568 (char 6946)\n",
      "âœ… DÃ­a 24/03/2025 procesado (2 noticias).\n",
      "ğŸ“¥ Se agregaron 21 noticias nuevas a Adrian_de_la_Garza.\n",
      "\n",
      "ğŸ” Procesando noticias de: Congreso_NL\n",
      "ğŸ“… Ãšltima fecha registrada: 19/03/2025\n",
      "âœ… DÃ­a 20/03/2025 procesado (7 noticias).\n",
      "âœ… DÃ­a 21/03/2025 procesado (6 noticias).\n",
      "âœ… DÃ­a 22/03/2025 procesado (2 noticias).\n",
      "âœ… DÃ­a 23/03/2025 procesado (5 noticias).\n",
      "âœ… DÃ­a 24/03/2025 procesado (1 noticias).\n",
      "ğŸ“¥ Se agregaron 21 noticias nuevas a Congreso_NL.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ğŸ”¹ Configurar el token de GitHub (para evitar bloqueos en API)\n",
    "os.environ[\"GH_TOKEN\"] = \"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\"\n",
    "\n",
    "# ğŸ”¹ Lista de periÃ³dicos permitidos\n",
    "PERIODICOS_PERMITIDOS = [\n",
    "    \"reporteindigo.com\", \"mvsnoticias.com\", \"lasillarota.com\",\n",
    "    \"milenio.com\", \"elnorte.com\", \"elhorizonte.mx\", \"elporvenir.mx\", \"abcnoticias.mx\"\n",
    "]\n",
    "\n",
    "# ğŸ”¹ FunciÃ³n para obtener URLs de noticias en una fecha especÃ­fica\n",
    "def buscar_noticias_fecha(query, dia, mes, anio):\n",
    "    url = f\"https://www.google.com/search?q={query}&tbs=cdr:1,cd_min:{mes}/{dia}/{anio},cd_max:{mes}/{dia}/{anio}&tbm=nws\"\n",
    "\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.add_argument(\"--headless\")  # Para que no abra la ventana del navegador\n",
    "\n",
    "    service = FirefoxService(GeckoDriverManager().install())\n",
    "    driver = webdriver.Firefox(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    urls_encontrados = []\n",
    "    while True:\n",
    "        noticias = driver.find_elements(By.CSS_SELECTOR, 'a')\n",
    "        for noticia in noticias:\n",
    "            enlace = noticia.get_attribute('href')\n",
    "            if enlace and any(periodico in enlace for periodico in PERIODICOS_PERMITIDOS):\n",
    "                urls_encontrados.append(enlace)\n",
    "\n",
    "        try:\n",
    "            siguiente_pagina = driver.find_element(By.LINK_TEXT, \"Siguiente\")\n",
    "            siguiente_pagina.send_keys(Keys.RETURN)\n",
    "            time.sleep(3)\n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return list(set(urls_encontrados))\n",
    "\n",
    "# ğŸ”¹ FunciÃ³n para extraer contenido de noticias segÃºn el medio\n",
    "def extraer_noticia(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        if \"milenio.com\" in url:\n",
    "            title = soup.find(\"title\").text.strip()\n",
    "            content = \"\\n\".join([p.text.strip() for p in soup.find_all(\"p\")])\n",
    "\n",
    "        elif \"elnorte.com\" in url or \"elhorizonte.mx\" in url or \"elporvenir.mx\" in url:\n",
    "            json_ld_script = soup.find(\"script\", type=\"application/ld+json\")\n",
    "            if json_ld_script:\n",
    "                json_data = json.loads(json_ld_script.string)\n",
    "                title = json_data.get(\"headline\", \"No encontrado\")\n",
    "                content = json_data.get(\"articleBody\", \"No disponible\")\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        elif \"reporteindigo.com\" in url:\n",
    "            title = soup.find(\"h1\").get_text(strip=True)\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs if len(p.text.strip()) > 10])\n",
    "\n",
    "        elif \"lasillarota.com\" in url or \"mvsnoticias.com\" in url:\n",
    "            title = soup.find(\"h1\").get_text(strip=True)\n",
    "            content = \" \".join([p.get_text(strip=True) for p in soup.find_all(\"p\")])\n",
    "            content = re.sub(r\"SÃGUENOS.*?GOOGLE NEWS\", \"\", content)\n",
    "\n",
    "        elif \"abcnoticias.mx\" in url:\n",
    "            title = soup.find(\"title\").text.strip()\n",
    "            paragraphs = soup.select(\"article p, .news-content p, [data-article-body] p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        return {\"url\": url, \"titulo\": title, \"contenido\": content}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error extrayendo {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def procesar_categoria(categoria, query):\n",
    "    print(f\"\\nğŸ” Procesando noticias de: {categoria}\")\n",
    "\n",
    "    # Cargar el CSV existente si existe\n",
    "    try:\n",
    "        df_existente = pd.read_csv(f\"noticias_{categoria}.csv\")\n",
    "        df_existente[\"fecha\"] = pd.to_datetime(df_existente[\"fecha\"], dayfirst=True, errors='coerce')\n",
    "        fecha_inicio = df_existente[\"fecha\"].max() + timedelta(days=1)\n",
    "        print(f\"ğŸ“… Ãšltima fecha registrada: {df_existente['fecha'].max().strftime('%d/%m/%Y')}\")\n",
    "    except FileNotFoundError:\n",
    "        df_existente = pd.DataFrame(columns=[\"fecha\", \"url\", \"titulo\", \"contenido\"])\n",
    "        fecha_inicio = datetime(2025, 1, 1)\n",
    "        print(\"ğŸ†• No se encontrÃ³ archivo previo. Se iniciarÃ¡ desde el 01/01/2025\")\n",
    "\n",
    "    fecha_actual = datetime.today()\n",
    "\n",
    "    # Si ya estÃ¡ al dÃ­a\n",
    "    if fecha_inicio.date() > fecha_actual.date():\n",
    "        print(\"âœ… Ya estÃ¡ actualizado. No hay fechas nuevas por procesar.\")\n",
    "        return\n",
    "\n",
    "    nuevas_noticias = []\n",
    "\n",
    "    for fecha_iter in pd.date_range(start=fecha_inicio, end=fecha_actual):\n",
    "        dia, mes, anio = fecha_iter.day, fecha_iter.month, fecha_iter.year\n",
    "\n",
    "        # Buscar noticias del dÃ­a\n",
    "        urls = buscar_noticias_fecha(query, dia, mes, anio)\n",
    "        noticias_del_dia = []\n",
    "\n",
    "        for url in urls:\n",
    "            if url not in df_existente[\"url\"].values:\n",
    "                noticia = extraer_noticia(url)\n",
    "                if noticia:\n",
    "                    noticias_del_dia.append({\n",
    "                        \"fecha\": fecha_iter.strftime(\"%d/%m/%Y\"),\n",
    "                        \"url\": noticia[\"url\"],\n",
    "                        \"titulo\": noticia[\"titulo\"],\n",
    "                        \"contenido\": noticia[\"contenido\"]\n",
    "                    })\n",
    "\n",
    "        nuevas_noticias.extend(noticias_del_dia)\n",
    "        print(f\"âœ… DÃ­a {fecha_iter.strftime('%d/%m/%Y')} procesado ({len(noticias_del_dia)} noticias).\")\n",
    "\n",
    "    # Guardar si hay nuevas noticias\n",
    "    if nuevas_noticias:\n",
    "        df_nuevas = pd.DataFrame(nuevas_noticias)\n",
    "        df_total = pd.concat([df_existente, df_nuevas], ignore_index=True)\n",
    "        df_total.to_csv(f\"noticias_{categoria}.csv\", index=False)\n",
    "        print(f\"ğŸ“¥ Se agregaron {len(nuevas_noticias)} noticias nuevas a {categoria}.\")\n",
    "    else:\n",
    "        print(f\"âš  No se encontraron noticias nuevas para {categoria}.\")\n",
    "\n",
    "\n",
    "# ğŸ”¹ Ejecutar recolecciÃ³n para cada categorÃ­a\n",
    "categorias = {\n",
    "    \"Monterrey\": \"Monterrey+noticias\",\n",
    "    \"Adrian_de_la_Garza\": \"Adrian+de+la+Garza+noticias\",\n",
    "    \"Congreso_NL\": \"Congreso+Nuevo+LeÃ³n+noticias\"\n",
    "}\n",
    "\n",
    "for categoria, query in categorias.items():\n",
    "    procesar_categoria(categoria, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2132c19-16cc-4f82-b285-308eb69fad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo Excel 'noticias_Monterrey.xlsx' actualizado con 2718 noticias.\n",
      "Archivo Excel 'noticias_Adrian_de_la_Garza.xlsx' actualizado con 444 noticias.\n",
      "Archivo Excel 'noticias_Congreso_NL.xlsx' actualizado con 560 noticias.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lista de nombres de CSV de cada categorÃ­a (ajusta los nombres si es necesario)\n",
    "csv_files = [\n",
    "    \"noticias_Monterrey.csv\",\n",
    "    \"noticias_Adrian_de_la_Garza.csv\",\n",
    "    \"noticias_Congreso_NL.csv\"\n",
    "]\n",
    "\n",
    "for archivo in csv_files:\n",
    "    try:\n",
    "        # Leer el CSV existente\n",
    "        df = pd.read_csv(archivo)\n",
    "        # Definir el nombre del archivo Excel, cambiando la extensiÃ³n .csv por .xlsx\n",
    "        excel_name = archivo.replace(\".csv\", \".xlsx\")\n",
    "        # Guardar el DataFrame en formato Excel\n",
    "        df.to_excel(excel_name, index=False)\n",
    "        print(f\"Archivo Excel '{excel_name}' actualizado con {len(df)} noticias.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error actualizando {archivo}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d093f1-f765-42a6-ab4a-a72d6a72522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv311/lib/python3.11/site-packages (4.50.0)\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in ./venv311/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./venv311/lib/python3.11/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv311/lib/python3.11/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv311/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv311/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv311/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./venv311/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv311/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv311/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv311/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv311/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./venv311/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv311/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv311/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv311/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv311/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv311/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv311/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading torch-2.6.0-cp311-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§  INSTALAR LIBRERÃAS NECESARIAS (solo la primera vez)\n",
    "import sys\n",
    "!{sys.executable} -m pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba60207-00b3-4ba3-b936-5c9fe153b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.43.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting vaderSentiment\n",
      "  Using cached vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Collecting deep-translator\n",
      "  Using cached deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in ./venv311/lib/python3.11/site-packages (from streamlit) (2.2.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in ./venv311/lib/python3.11/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in ./venv311/lib/python3.11/site-packages (from streamlit) (2.2.3)\n",
      "Collecting pillow<12,>=7.1.0 (from streamlit)\n",
      "  Downloading pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Using cached protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in ./venv311/lib/python3.11/site-packages (from streamlit) (2.32.3)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in ./venv311/lib/python3.11/site-packages (from streamlit) (4.12.2)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in ./venv311/lib/python3.11/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in ./venv311/lib/python3.11/site-packages (from deep-translator) (4.13.3)\n",
      "Requirement already satisfied: jinja2 in ./venv311/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in ./venv311/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.32.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv311/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv311/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv311/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv311/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv311/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./venv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Using cached streamlit-1.43.2-py2.py3-none-any.whl (9.7 MB)\n",
      "Using cached vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Using cached deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading narwhals-1.32.0-py3-none-any.whl (320 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: toml, tenacity, smmap, pyarrow, protobuf, pillow, narwhals, click, cachetools, blinker, vaderSentiment, pydeck, gitdb, deep-translator, gitpython, altair, streamlit\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-5.5.2 click-8.1.8 deep-translator-1.11.4 gitdb-4.0.12 gitpython-3.1.44 narwhals-1.32.0 pillow-11.1.0 protobuf-5.29.4 pyarrow-19.0.1 pydeck-0.9.1 smmap-5.0.2 streamlit-1.43.2 tenacity-9.0.0 toml-0.10.2 vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit vaderSentiment deep-translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be73562-8846-43d2-b985-2a8784e4693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ğŸ“¦ IMPORTACIONES\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "from diccionarios_monterrey import *\n",
    "\n",
    "tqdm.pandas()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# ğŸ­ CARGAR MODELO DE EMOCIONES MÃšLTIPLES\n",
    "clasificador_emociones = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"finiteautomata/beto-emotion-analysis\",\n",
    "    top_k=1\n",
    ")\n",
    "\n",
    "# ğŸ“Š FUNCIONES DE CLASIFICACIÃ“N\n",
    "PALABRAS_NEGATIVAS = [\n",
    "    \"balean\", \"asesinato\", \"homicidio\", \"muere\", \"muerto\", \"emboscada\",\n",
    "    \"violencia\", \"disparos\", \"balacera\", \"fallece\", \"mala calidad del aire\",\n",
    "    \"inseguridad\", \"crimen\", \"atacan\", \"tiroteo\"\n",
    "]\n",
    "\n",
    "PALABRAS_POSITIVAS = [\n",
    "    \"reconocen\", \"premian\", \"mejoran\", \"avance\", \"logro\", \"beneficio\", \"positivo\",\n",
    "    \"descuento\", \"apoyo\", \"felicitaciÃ³n\", \"entrega de apoyos\", \"lograron\", \"inauguran\", \n",
    "    \"expansiÃ³n\", \"desarrollo\", \"mejora\", \"crecimiento\", \"reactivaciÃ³n econÃ³mica\"\n",
    "]\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    reemplazos = {\n",
    "        \"ÃƒÂ³\": \"Ã³\", \"ÃƒÂº\": \"Ãº\", \"ÃƒÂ¡\": \"Ã¡\", \"ÃƒÂn\": \"Ã\", \"ÃƒÂ­\": \"Ã­\", \"ÃƒÂ©\": \"Ã©\", \"ÃƒÂ±\": \"Ã±\",\n",
    "        \"âˆšÃ‡Â¬Ã¸\": \"Â¿\", \"âˆšÃ‰Â¬Â°\": \"Ã¡\", \"âˆšÃ‰\": \"Ã©\", \"âˆšÃ‰Â¬Â©\": \"Ã©\", \"âˆšÃ‰Â¬Â±\": \"Ã±\", \"âˆšÃ‰Â¬â‰¥\": \"Ã³\",\n",
    "        \"âˆšÃ‡Â¬Â°\": \"Â¡\", \"âˆšÃ‰Â¬â‰ \": \"Ã­\", \"âˆšÃ‰Â¬âˆ«\": \"Ãº\", \"Ã‚Â¿\": \"Â¿\", \"Ã‚Â¡\": \"Â¡\"\n",
    "    }\n",
    "    for error, correcto in reemplazos.items():\n",
    "        texto = texto.replace(error, correcto)\n",
    "    return texto\n",
    "\n",
    "def contiene_monterrey(texto):\n",
    "    texto = texto.lower()\n",
    "    return any(p in texto for p in [\n",
    "        \"monterrey\", \"regio\", \"regia\", \"regiomontano\", \"regiomontana\",\n",
    "        \"zmm\", \"amm\", \"Ã¡rea metropolitana\", \"zona metropolitana\", \"metropolitana\", \"metropolitanos\"\n",
    "    ])\n",
    "\n",
    "def clasificar_noticia(texto):\n",
    "    texto = texto.lower()\n",
    "\n",
    "    if not contiene_monterrey(texto):\n",
    "        return (None, None, None)\n",
    "\n",
    "    if any(p in texto for p in [\"yuri\", \"fichaje\", \"fichajes\",\"sears\",\"conciertos\",\"concierto\"]):\n",
    "        return (None, None, None)\n",
    "    if \"samuel\" in texto and \"adriÃ¡n\" not in texto:\n",
    "        return (None, None, None)\n",
    "\n",
    "    if any(x in texto for x in [\"dif-\", \"gabriela oyervides\", \"gaby oyervides\", \"xÃ³chitl loredo\", \"dif \", \"dif.\", \"dif,\",\"dif municipal\", \"dif de monterrey\"]):\n",
    "        return (\"Gobierno\", \"DIF Mty\", \"dif\")\n",
    "    \n",
    "    if any(x in texto for x in [\"maday\", \"prepa regia\", \"juventud regia\", \"injure\", \"jÃ³venes\", \"juventud\", \"maday cantÃº\"]):\n",
    "        return (\"Gobierno\", \"INJURE\", \"injure\")\n",
    "\n",
    "    if any(x in texto for x in [\"nazario pineda\", \"infraestructura sostenible\", \"obras pÃºblicas\", \"obra pÃºblica\" \"construcciÃ³n\"]):\n",
    "        return (\"Gobierno\", \"Infraestructura Sostenible\", \"infraestructura\")\n",
    "\n",
    "    if any(x in texto for x in [\"presidente municipal\", \"alcalde de monterrey\", \"adriÃ¡n de la garza\", \"adriÃ¡n dlg\"]):\n",
    "        return (\"Alcalde\", None, \"alcalde\")\n",
    "\n",
    "    if \"congreso\" in texto:\n",
    "        return (\"Congreso\", None, \"congreso\")\n",
    "\n",
    "    for palabra in diccionario_indices_inseguridad.get(\"obligatorias\", []) + diccionario_indices_inseguridad.get(\"relevantes\", []):\n",
    "        if palabra.lower() in texto:\n",
    "            return (\"Seguridad\", None, palabra)\n",
    "\n",
    "    sub_dicts_prioridad = [\n",
    "        (\"Ejecutiva\", diccionario_secretaria_ejecutiva),\n",
    "        (\"Ayuntamiento\", diccionario_ayuntamiento),\n",
    "        (\"ContralorÃ­a\", diccionario_contraloria),\n",
    "        (\"Seguridad y ProtecciÃ³n Ciudadana\", diccionario_seguridad_institucional),\n",
    "        (\"Desarrollo EconÃ³mico\", diccionario_desarrollo_economico),\n",
    "        (\"Servicios PÃºblicos\", diccionario_servicios_publicos),\n",
    "        (\"Desarrollo Urbano\", diccionario_desarrollo_urbano),\n",
    "        (\"Infraestructura Sostenible\", diccionario_infraestructura_sostenible),\n",
    "        (\"Desarrollo Humano e Igualdad Sustantiva\", diccionario_desarrollo_humano),\n",
    "        (\"InnovaciÃ³n y Gobierno Abierto\", diccionario_siga),\n",
    "        (\"DIF Mty\", diccionario_dif),\n",
    "        (\"IMMR\", diccionario_mujeres_regias),\n",
    "        (\"INJURE\", diccionario_juventud_regia),\n",
    "        (\"IMPLANC\", diccionario_implanc),\n",
    "    ]\n",
    "\n",
    "    for sub, dic in sub_dicts_prioridad:\n",
    "        for palabra in dic.get(\"obligatorias\", []) + dic.get(\"relevantes\", []):\n",
    "            if palabra.lower() in texto:\n",
    "                return (\"Gobierno\", sub, palabra)\n",
    "\n",
    "    return (None, None, None)\n",
    "\n",
    "def clasificar_sentimiento(texto): \n",
    "    texto_lower = texto.lower()\n",
    "    score = analyzer.polarity_scores(texto)[\"compound\"]\n",
    "\n",
    "    if any(p in texto_lower for p in PALABRAS_NEGATIVAS):\n",
    "        return \"ğŸ”´\"\n",
    "    if any(p in texto_lower for p in PALABRAS_POSITIVAS):\n",
    "        return \"ğŸŸ¢\"\n",
    "    if score <= -0.4:\n",
    "        return \"ğŸ”´\"\n",
    "    elif score >= 0.4:\n",
    "        return \"ğŸŸ¢\"\n",
    "    else:\n",
    "        return \"ğŸŸ¡\"\n",
    "\n",
    "def clasificar_emocion(texto):\n",
    "    try:\n",
    "        resultado = clasificador_emociones(texto)\n",
    "        return resultado[0][0]['label']\n",
    "    except:\n",
    "        return \"no_clasificado\"\n",
    "\n",
    "MAPA_MEDIOS = {\n",
    "    \"reporteindigo.com\": \"reporteindigo\",\n",
    "    \"mvsnoticias.com\": \"mvsnoticias\",\n",
    "    \"lasillarota.com\": \"lasillarota\",\n",
    "    \"milenio.com\": \"milenio\",\n",
    "    \"elnorte.com\": \"elnorte\",\n",
    "    \"elhorizonte.mx\": \"elhorizonte\",\n",
    "    \"elporvenir.mx\": \"elporvenir\",\n",
    "    \"abcnoticias.mx\": \"abcnoticias\"\n",
    "}\n",
    "\n",
    "def extraer_medio(url):\n",
    "    for dominio, nombre in MAPA_MEDIOS.items():\n",
    "        if dominio in url:\n",
    "            return f\"#{nombre}\"\n",
    "    return \"#medio_desconocido\"\n",
    "\n",
    "# ğŸš€ FUNCIÃ“N PRINCIPAL\n",
    "def procesar_noticias():\n",
    "    archivos = [\n",
    "        \"noticias_Adrian_de_la_Garza.csv\",\n",
    "        \"noticias_Congreso_NL.csv\",\n",
    "        \"noticias_Monterrey.csv\"\n",
    "    ]\n",
    "\n",
    "    dfs = []\n",
    "    for archivo in archivos:\n",
    "        print(f\"ğŸ“„ Leyendo archivo: {archivo}\")\n",
    "        try:\n",
    "            df = pd.read_csv(archivo)\n",
    "            print(f\"âœ… {archivo} cargado con {len(df)} filas\")\n",
    "            for col in [\"titulo\", \"contenido\"]:\n",
    "                df[col] = df[col].astype(str).apply(limpiar_texto)\n",
    "            dfs.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âŒ Archivo NO encontrado: {archivo}\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"âš ï¸ No se cargÃ³ ningÃºn archivo CSV\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    noticias = pd.concat(dfs, ignore_index=True)\n",
    "    noticias[\"texto_completo\"] = noticias[\"titulo\"].astype(str) + \" \" + noticias[\"contenido\"].astype(str)\n",
    "\n",
    "    print(\"ğŸ·ï¸ Clasificando noticias...\")\n",
    "    noticias[[\"pestaÃ±a\", \"subpestaÃ±a\", \"hashtag_diccionario\"]] = noticias[\"texto_completo\"].apply(\n",
    "        lambda x: pd.Series(clasificar_noticia(x))\n",
    "    )\n",
    "    noticias = noticias[noticias[\"pestaÃ±a\"].notna()]\n",
    "\n",
    "    print(\"ğŸ” Analizando sentimientos...\")\n",
    "    noticias[\"sentimiento\"] = noticias[\"texto_completo\"].progress_apply(clasificar_sentimiento)\n",
    "\n",
    "    print(\"ğŸ­ Analizando emociones mÃºltiples...\")\n",
    "    noticias[\"emocion\"] = noticias[\"texto_completo\"].progress_apply(clasificar_emocion)\n",
    "\n",
    "    print(\"ğŸ·ï¸ Extrayendo hashtags...\")\n",
    "    noticias[\"hashtag_medio\"] = noticias[\"url\"].progress_apply(extraer_medio)\n",
    "    noticias[\"hashtag_diccionario\"] = noticias[\"hashtag_diccionario\"].apply(\n",
    "        lambda x: f\"#{x.lower().replace(' ', '_')}\" if pd.notnull(x) else \"#sin_etiqueta\"\n",
    "    )\n",
    "\n",
    "    print(\"ğŸ“… Formateando fechas...\")\n",
    "    from dateutil import parser\n",
    "\n",
    "    def parsear_fecha(fecha):\n",
    "        try:\n",
    "            return parser.parse(str(fecha), dayfirst=True, fuzzy=True)\n",
    "        except:\n",
    "            return pd.NaT\n",
    "    \n",
    "    print(\"ğŸ“… Corrigiendo fechas...\")\n",
    "    noticias[\"fecha\"] = noticias[\"fecha\"].apply(parsear_fecha)\n",
    "    fechas_invalidas = noticias[\"fecha\"].isna().sum()\n",
    "    \n",
    "    if fechas_invalidas > 0:\n",
    "        print(f\"âš ï¸ {fechas_invalidas} noticias tienen fecha invÃ¡lida y se conservarÃ¡n como NaT\")\n",
    "    else:\n",
    "        print(\"âœ… Todas las fechas fueron procesadas correctamente.\")\n",
    "    \n",
    "\n",
    "    print(f\"âœ… Noticias procesadas completamente: {len(noticias)} registros\")\n",
    "    noticias.to_csv(\"noticias_procesadas.csv\", index=False)\n",
    "\n",
    "    return noticias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f11eec4b-a391-42b4-8fcd-a6905ce2e167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Leyendo archivo: noticias_Adrian_de_la_Garza.csv\n",
      "âœ… noticias_Adrian_de_la_Garza.csv cargado con 444 filas\n",
      "ğŸ“„ Leyendo archivo: noticias_Congreso_NL.csv\n",
      "âœ… noticias_Congreso_NL.csv cargado con 560 filas\n",
      "ğŸ“„ Leyendo archivo: noticias_Monterrey.csv\n",
      "âœ… noticias_Monterrey.csv cargado con 2718 filas\n",
      "ğŸ·ï¸ Clasificando noticias...\n",
      "ğŸ” Analizando sentimientos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2011/2011 [00:01<00:00, 1056.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ­ Analizando emociones mÃºltiples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "00%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2011/2011 [05:21<00:00,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ·ï¸ Extrayendo hashtags...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2011/2011 [00:00<00:00, 498625.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… Formateando fechas...\n",
      "ğŸ“… Corrigiendo fechas...\n",
      "âœ… Todas las fechas fueron procesadas correctamente.\n",
      "âœ… Noticias procesadas completamente: 2011 registros\n"
     ]
    }
   ],
   "source": [
    "noticias = procesar_noticias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d550a043-d24e-4c89-8432-063c7b5912cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "00%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2011/2011 [05:17<00:00,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Guardado como noticias_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "from diccionarios_monterrey import *\n",
    "from dateutil import parser\n",
    "\n",
    "tqdm.pandas()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "clasificador_emociones = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"finiteautomata/beto-emotion-analysis\",\n",
    "    top_k=6,\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "def analizar_emociones(texto):\n",
    "    try:\n",
    "        resultados = clasificador_emociones(texto)\n",
    "        scores_list = resultados[0]\n",
    "        return {item['label']: item['score'] for item in scores_list}\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "def puntaje_sentimiento(texto):\n",
    "    return analyzer.polarity_scores(texto)[\"compound\"]\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    reemplazos = {\n",
    "        \"ÃƒÂ³\": \"Ã³\", \"ÃƒÂº\": \"Ãº\", \"ÃƒÂ¡\": \"Ã¡\", \"ÃƒÂn\": \"Ã\", \"ÃƒÂ­\": \"Ã­\", \"ÃƒÂ©\": \"Ã©\", \"ÃƒÂ±\": \"Ã±\",\n",
    "        \"âˆšÃ‡Â¬Ã¸\": \"Â¿\", \"âˆšÃ‰Â¬Â°\": \"Ã¡\", \"âˆšÃ‰\": \"Ã©\", \"âˆšÃ‰Â¬Â©\": \"Ã©\", \"âˆšÃ‰Â¬Â±\": \"Ã±\", \"âˆšÃ‰Â¬â‰¥\": \"Ã³\",\n",
    "        \"âˆšÃ‡Â¬Â°\": \"Â¡\", \"âˆšÃ‰Â¬â‰ \": \"Ã­\", \"âˆšÃ‰Â¬âˆ«\": \"Ãº\", \"Ã‚Â¿\": \"Â¿\", \"Ã‚Â¡\": \"Â¡\"\n",
    "    }\n",
    "    for error, correcto in reemplazos.items():\n",
    "        texto = texto.replace(error, correcto)\n",
    "    return texto\n",
    "\n",
    "def contiene_monterrey(texto):\n",
    "    texto = texto.lower()\n",
    "    return any(p in texto for p in [\n",
    "        \"monterrey\", \"regio\", \"regia\", \"regiomontano\", \"regiomontana\",\n",
    "        \"zmm\", \"amm\", \"Ã¡rea metropolitana\", \"zona metropolitana\", \"metropolitana\", \"metropolitanos\"\n",
    "    ])\n",
    "\n",
    "def clasificar_noticia(texto):\n",
    "    texto = texto.lower()\n",
    "\n",
    "    if not contiene_monterrey(texto):\n",
    "        return (None, None, None)\n",
    "\n",
    "    if any(p in texto for p in [\"yuri\", \"fichaje\", \"fichajes\", \"sears\", \"conciertos\", \"concierto\"]):\n",
    "        return (None, None, None)\n",
    "    if \"samuel\" in texto and \"adriÃ¡n\" not in texto:\n",
    "        return (None, None, None)\n",
    "\n",
    "    if any(x in texto for x in [\"dif-\", \"gabriela oyervides\", \"gaby oyervides\", \"xÃ³chitl loredo\", \n",
    "                                \"dif \", \"dif.\", \"dif,\", \"dif municipal\", \"dif de monterrey\"]):\n",
    "        return (\"Gobierno\", \"DIF Mty\", \"dif\")\n",
    "\n",
    "    if any(x in texto for x in [\"maday\", \"prepa regia\", \"juventud regia\", \"injure\", \"jÃ³venes\", \"juventud\", \"maday cantÃº\"]):\n",
    "        return (\"Gobierno\", \"INJURE\", \"injure\")\n",
    "\n",
    "    if any(x in texto for x in [\"nazario pineda\", \"infraestructura sostenible\", \"obras pÃºblicas\", \"obra pÃºblica\", \"construcciÃ³n\"]):\n",
    "        return (\"Gobierno\", \"Infraestructura Sostenible\", \"infraestructura\")\n",
    "\n",
    "    if any(x in texto for x in [\"presidente municipal\", \"alcalde de monterrey\", \"adriÃ¡n de la garza\", \"adriÃ¡n dlg\"]):\n",
    "        return (\"Alcalde\", None, \"alcalde\")\n",
    "\n",
    "    if \"congreso\" in texto:\n",
    "        return (\"Congreso\", None, \"congreso\")\n",
    "\n",
    "    for palabra in diccionario_indices_inseguridad.get(\"obligatorias\", []) + diccionario_indices_inseguridad.get(\"relevantes\", []):\n",
    "        if palabra.lower() in texto:\n",
    "            return (\"Seguridad\", None, palabra)\n",
    "\n",
    "    sub_dicts_prioridad = [\n",
    "        (\"Ejecutiva\", diccionario_secretaria_ejecutiva),\n",
    "        (\"Ayuntamiento\", diccionario_ayuntamiento),\n",
    "        (\"ContralorÃ­a\", diccionario_contraloria),\n",
    "        (\"Seguridad y ProtecciÃ³n Ciudadana\", diccionario_seguridad_institucional),\n",
    "        (\"Desarrollo EconÃ³mico\", diccionario_desarrollo_economico),\n",
    "        (\"Servicios PÃºblicos\", diccionario_servicios_publicos),\n",
    "        (\"Desarrollo Urbano\", diccionario_desarrollo_urbano),\n",
    "        (\"Infraestructura Sostenible\", diccionario_infraestructura_sostenible),\n",
    "        (\"Desarrollo Humano e Igualdad Sustantiva\", diccionario_desarrollo_humano),\n",
    "        (\"InnovaciÃ³n y Gobierno Abierto\", diccionario_siga),\n",
    "        (\"DIF Mty\", diccionario_dif),\n",
    "        (\"IMMR\", diccionario_mujeres_regias),\n",
    "        (\"INJURE\", diccionario_juventud_regia),\n",
    "        (\"IMPLANC\", diccionario_implanc),\n",
    "    ]\n",
    "\n",
    "    for sub, dic in sub_dicts_prioridad:\n",
    "        for palabra in dic.get(\"obligatorias\", []) + dic.get(\"relevantes\", []):\n",
    "            if palabra.lower() in texto:\n",
    "                return (\"Gobierno\", sub, palabra)\n",
    "\n",
    "    return (None, None, None)\n",
    "\n",
    "def extraer_medio(url):\n",
    "    MAPA_MEDIOS = {\n",
    "        \"reporteindigo.com\": \"reporteindigo\",\n",
    "        \"mvsnoticias.com\": \"mvsnoticias\",\n",
    "        \"lasillarota.com\": \"lasillarota\",\n",
    "        \"milenio.com\": \"milenio\",\n",
    "        \"elnorte.com\": \"elnorte\",\n",
    "        \"elhorizonte.mx\": \"elhorizonte\",\n",
    "        \"elporvenir.mx\": \"elporvenir\",\n",
    "        \"abcnoticias.mx\": \"abcnoticias\"\n",
    "    }\n",
    "    for dominio, nombre in MAPA_MEDIOS.items():\n",
    "        if dominio in url:\n",
    "            return f\"#{nombre}\"\n",
    "    return \"#medio_desconocido\"\n",
    "\n",
    "def parsear_fecha(fecha):\n",
    "    try:\n",
    "        return parser.parse(str(fecha), dayfirst=True, fuzzy=True)\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "# Procesar archivo original\n",
    "archivo_entrada = \"noticias_procesadas.csv\"\n",
    "df = pd.read_csv(archivo_entrada)\n",
    "\n",
    "for col in [\"titulo\", \"contenido\"]:\n",
    "    df[col] = df[col].astype(str).apply(limpiar_texto)\n",
    "\n",
    "df[\"texto_completo\"] = df[\"titulo\"] + \" \" + df[\"contenido\"]\n",
    "df[[\"pestaÃ±a\", \"subpestaÃ±a\", \"hashtag_diccionario\"]] = df[\"texto_completo\"].apply(lambda x: pd.Series(clasificar_noticia(x)))\n",
    "df = df[df[\"pestaÃ±a\"].notna()]\n",
    "\n",
    "df[\"sentimiento_vader\"] = df[\"texto_completo\"].progress_apply(puntaje_sentimiento)\n",
    "\n",
    "df[\"emociones\"] = df[\"texto_completo\"].progress_apply(analizar_emociones)\n",
    "df_emociones = df[\"emociones\"].apply(pd.Series)\n",
    "df = pd.concat([df, df_emociones], axis=1)\n",
    "\n",
    "# Columna emociÃ³n principal y su score\n",
    "def emocion_principal(emociones_dict):\n",
    "    if not emociones_dict:\n",
    "        return pd.Series({\"emocion_principal\": \"no_clasificado\", \"score_emocion\": 0})\n",
    "    emocion, score = max(emociones_dict.items(), key=lambda x: x[1])\n",
    "    return pd.Series({\"emocion_principal\": emocion, \"score_emocion\": score})\n",
    "\n",
    "df[[\"emocion_principal\", \"score_emocion\"]] = df[\"emociones\"].apply(emocion_principal)\n",
    "\n",
    "# Emojis claros de positivo, negativo o neutro\n",
    "df[\"emocion_sentimiento\"] = df[\"emocion_principal\"].map({\n",
    "    \"joy\":\"ğŸŸ¢\",\"love\":\"ğŸŸ¢\",\"anger\":\"ğŸ”´\",\"sadness\":\"ğŸ”´\",\"fear\":\"ğŸ”´\",\"disgust\":\"ğŸ”´\",\n",
    "    \"others\":\"ğŸŸ¡\",\"surprise\":\"ğŸŸ¡\",\"neutral\":\"ğŸŸ¡\"\n",
    "}).fillna(\"ğŸŸ¡\")\n",
    "\n",
    "df[\"hashtag_medio\"] = df[\"url\"].apply(extraer_medio)\n",
    "df[\"fecha\"] = df[\"fecha\"].apply(parsear_fecha).dropna()\n",
    "\n",
    "df.to_csv(\"noticias_final.csv\",index=False)\n",
    "print(\"âœ… Guardado como noticias_final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "279f5ab6-d0bb-4158-ab72-25cae6ced305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv311/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: streamlit in ./venv311/lib/python3.11/site-packages (1.43.2)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./venv311/lib/python3.11/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv311/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv311/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv311/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in ./venv311/lib/python3.11/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in ./venv311/lib/python3.11/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in ./venv311/lib/python3.11/site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in ./venv311/lib/python3.11/site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: packaging<25,>=20 in ./venv311/lib/python3.11/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in ./venv311/lib/python3.11/site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in ./venv311/lib/python3.11/site-packages (from streamlit) (5.29.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in ./venv311/lib/python3.11/site-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in ./venv311/lib/python3.11/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in ./venv311/lib/python3.11/site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in ./venv311/lib/python3.11/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in ./venv311/lib/python3.11/site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in ./venv311/lib/python3.11/site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in ./venv311/lib/python3.11/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in ./venv311/lib/python3.11/site-packages (from streamlit) (6.4.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jinja2 in ./venv311/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in ./venv311/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in ./venv311/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (1.32.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv311/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: six>=1.5 in ./venv311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv311/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./venv311/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv311/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./venv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
      "Downloading matplotlib-3.10.1-cp311-cp311-macosx_11_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading contourpy-1.3.1-cp311-cp311-macosx_11_0_arm64.whl (254 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "Downloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading pyparsing-3.2.2-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pyparsing-3.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas streamlit matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "309d3a01-d6d9-4214-8356-83840866ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysentimiento\n",
      "  Using cached pysentimiento-0.7.3-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting accelerate>=0.27.2 (from pysentimiento)\n",
      "  Using cached accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets>=2.10.1 (from pysentimiento)\n",
      "  Using cached datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting emoji>=1.6.1 (from pysentimiento)\n",
      "  Using cached emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting spacy>=3.5.0 (from pysentimiento)\n",
      "  Downloading spacy-3.8.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: torch!=2.0.1,>=2.0.0 in ./venv311/lib/python3.11/site-packages (from pysentimiento) (2.6.0)\n",
      "Requirement already satisfied: transformers>=4.13.0 in ./venv311/lib/python3.11/site-packages (from pysentimiento) (4.50.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./venv311/lib/python3.11/site-packages (from accelerate>=0.27.2->pysentimiento) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv311/lib/python3.11/site-packages (from accelerate>=0.27.2->pysentimiento) (24.2)\n",
      "Requirement already satisfied: psutil in ./venv311/lib/python3.11/site-packages (from accelerate>=0.27.2->pysentimiento) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./venv311/lib/python3.11/site-packages (from accelerate>=0.27.2->pysentimiento) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./venv311/lib/python3.11/site-packages (from accelerate>=0.27.2->pysentimiento) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv311/lib/python3.11/site-packages (from accelerate>=0.27.2->pysentimiento) (0.5.3)\n",
      "Requirement already satisfied: filelock in ./venv311/lib/python3.11/site-packages (from datasets>=2.10.1->pysentimiento) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv311/lib/python3.11/site-packages (from datasets>=2.10.1->pysentimiento) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.10.1->pysentimiento)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in ./venv311/lib/python3.11/site-packages (from datasets>=2.10.1->pysentimiento) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv311/lib/python3.11/site-packages (from datasets>=2.10.1->pysentimiento) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv311/lib/python3.11/site-packages (from datasets>=2.10.1->pysentimiento) (4.67.1)\n",
      "Collecting xxhash (from datasets>=2.10.1->pysentimiento)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.10.1->pysentimiento)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.10.1->pysentimiento)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets>=2.10.1->pysentimiento)\n",
      "  Downloading aiohttp-3.11.14-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy>=3.5.0->pysentimiento)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy>=3.5.0->pysentimiento)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy>=3.5.0->pysentimiento)\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy>=3.5.0->pysentimiento)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy>=3.5.0->pysentimiento)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy>=3.5.0->pysentimiento)\n",
      "  Downloading thinc-8.3.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy>=3.5.0->pysentimiento)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy>=3.5.0->pysentimiento)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy>=3.5.0->pysentimiento)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy>=3.5.0->pysentimiento)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy>=3.5.0->pysentimiento)\n",
      "  Using cached typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy>=3.5.0->pysentimiento)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: jinja2 in ./venv311/lib/python3.11/site-packages (from spacy>=3.5.0->pysentimiento) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./venv311/lib/python3.11/site-packages (from spacy>=3.5.0->pysentimiento) (75.6.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy>=3.5.0->pysentimiento)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv311/lib/python3.11/site-packages (from torch!=2.0.1,>=2.0.0->pysentimiento) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv311/lib/python3.11/site-packages (from torch!=2.0.1,>=2.0.0->pysentimiento) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv311/lib/python3.11/site-packages (from torch!=2.0.1,>=2.0.0->pysentimiento) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv311/lib/python3.11/site-packages (from sympy==1.13.1->torch!=2.0.1,>=2.0.0->pysentimiento) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv311/lib/python3.11/site-packages (from transformers>=4.13.0->pysentimiento) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv311/lib/python3.11/site-packages (from transformers>=4.13.0->pysentimiento) (0.21.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.10.1->pysentimiento)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.10.1->pysentimiento)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv311/lib/python3.11/site-packages (from aiohttp->datasets>=2.10.1->pysentimiento) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.10.1->pysentimiento)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.10.1->pysentimiento)\n",
      "  Downloading multidict-6.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.10.1->pysentimiento)\n",
      "  Downloading propcache-0.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.10.1->pysentimiento)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy>=3.5.0->pysentimiento)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.5.0->pysentimiento)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.5.0->pysentimiento)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv311/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.10.1->pysentimiento) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv311/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.10.1->pysentimiento) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv311/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.10.1->pysentimiento) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv311/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.10.1->pysentimiento) (2025.1.31)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy>=3.5.0->pysentimiento)\n",
      "  Downloading blis-1.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy>=3.5.0->pysentimiento)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in ./venv311/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.5.0->pysentimiento) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy>=3.5.0->pysentimiento)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy>=3.5.0->pysentimiento)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy>=3.5.0->pysentimiento)\n",
      "  Using cached cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy>=3.5.0->pysentimiento)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv311/lib/python3.11/site-packages (from jinja2->spacy>=3.5.0->pysentimiento) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv311/lib/python3.11/site-packages (from pandas->datasets>=2.10.1->pysentimiento) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv311/lib/python3.11/site-packages (from pandas->datasets>=2.10.1->pysentimiento) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv311/lib/python3.11/site-packages (from pandas->datasets>=2.10.1->pysentimiento) (2025.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.5.0->pysentimiento)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./venv311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.10.1->pysentimiento) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.5.0->pysentimiento)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv311/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.5.0->pysentimiento) (2.19.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.5.0->pysentimiento)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.5.0->pysentimiento)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached pysentimiento-0.7.3-py3-none-any.whl (39 kB)\n",
      "Using cached accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
      "Using cached datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Using cached emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "Downloading spacy-3.8.4-cp311-cp311-macosx_11_0_arm64.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.14-cp311-cp311-macosx_11_0_arm64.whl (456 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading murmurhash-1.0.12-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl (128 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-macosx_11_0_arm64.whl (634 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m634.4/634.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading thinc-8.3.4-cp311-cp311-macosx_11_0_arm64.whl (774 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m774.2/774.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.2.0-cp311-cp311-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-macosx_11_0_arm64.whl (52 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Downloading multidict-6.2.0-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading propcache-0.3.0-cp311-cp311-macosx_11_0_arm64.whl (45 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-macosx_11_0_arm64.whl (92 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-macosx_11_0_arm64.whl (174 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, xxhash, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, pydantic-core, propcache, murmurhash, multidict, mdurl, marisa-trie, fsspec, frozenlist, emoji, dill, cloudpathlib, catalogue, blis, annotated-types, aiohappyeyeballs, yarl, srsly, smart-open, pydantic, preshed, multiprocess, markdown-it-py, language-data, aiosignal, rich, langcodes, confection, aiohttp, accelerate, typer, thinc, weasel, datasets, spacy, pysentimiento\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "Successfully installed accelerate-1.5.2 aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 datasets-3.4.1 dill-0.3.8 emoji-2.14.1 frozenlist-1.5.0 fsspec-2024.12.0 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.2.0 multiprocess-0.70.16 murmurhash-1.0.12 preshed-3.0.9 propcache-0.3.0 pydantic-2.10.6 pydantic-core-2.27.2 pysentimiento-0.7.3 rich-13.9.4 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pysentimiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d3b84-d65e-4d14-a5c0-fb0fe6af50dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (venv311)",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
